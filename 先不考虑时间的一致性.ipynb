{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n"
   ],
   "id": "df87b1188729cb68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def check_invalid_values(image_path):\n",
    "    \"\"\"\n",
    "    检查单张影像是否存在无效值，并统计无效值比例。\n",
    "    :param image_path: 影像文件路径\n",
    "    :return: 无效值比例字典，键为波段索引，值为无效值比例\n",
    "    \"\"\"\n",
    "    invalid_ratios = {}\n",
    "    try:\n",
    "        with rasterio.open(image_path) as src:\n",
    "            data = src.read()  # 读取影像数据 (C, H, W)\n",
    "            num_bands, height, width = data.shape\n",
    "\n",
    "            for band_idx in range(num_bands):\n",
    "                band_data = data[band_idx]\n",
    "                total_pixels = height * width\n",
    "                invalid_pixels = np.isnan(band_data).sum() + np.isinf(band_data).sum()\n",
    "                invalid_ratio = invalid_pixels / total_pixels\n",
    "                invalid_ratios[band_idx + 1] = invalid_ratio  # 波段从1开始编号\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return invalid_ratios\n",
    "\n",
    "\n",
    "def analyze_dataset(root_dir, output_log):\n",
    "    \"\"\"\n",
    "    分析整个数据集，统计每张影像的无效值比例，并将结果写入日志文件。\n",
    "    :param root_dir: 数据集根目录\n",
    "    :param output_log: 输出日志文件路径\n",
    "    \"\"\"\n",
    "    invalid_images = []  # 存储包含无效值的影像路径\n",
    "    total_images = 0  # 总影像数\n",
    "\n",
    "    # 打开日志文件以写入模式\n",
    "    with open(output_log, \"w\") as log_file:\n",
    "        log_file.write(\"Invalid Value Analysis Report\\n\")\n",
    "        log_file.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        # 遍历目录中的所有 .tif 文件\n",
    "        for dirpath, _, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.lower().endswith('.tif'):\n",
    "                    total_images += 1\n",
    "                    image_path = os.path.join(dirpath, filename)\n",
    "                    invalid_ratios = check_invalid_values(image_path)\n",
    "\n",
    "                    if invalid_ratios is not None and any(ratio > 0 for ratio in invalid_ratios.values()):\n",
    "                        invalid_images.append((image_path, invalid_ratios))\n",
    "                        log_file.write(f\"Invalid values found in {image_path}:\\n\")\n",
    "                        for band, ratio in invalid_ratios.items():\n",
    "                            log_file.write(f\"  Band {band}: {ratio:.2%} invalid pixels\\n\")\n",
    "\n",
    "        # 写入总结信息\n",
    "        log_file.write(\"\\nAnalysis Summary:\\n\")\n",
    "        log_file.write(f\"Total images analyzed: {total_images}\\n\")\n",
    "        log_file.write(f\"Images with invalid values: {len(invalid_images)}\\n\")\n",
    "        if invalid_images:\n",
    "            log_file.write(\"List of images with invalid values:\\n\")\n",
    "            for path, _ in invalid_images:\n",
    "                log_file.write(f\"  {path}\\n\")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "# s1_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S1_processed_cropped_index\"\n",
    "s2_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S2\\ValidImages\"\n",
    "\n",
    "# 定义日志文件路径\n",
    "# s1_log_file = os.path.join(s1_image_dir, \"S1_invalid_values.txt\")\n",
    "s2_log_file = os.path.join(s2_image_dir, \"S2_invalid_values.txt\")\n",
    "\n",
    "# print(\"Analyzing S1 images...\")\n",
    "# analyze_dataset(s1_image_dir, s1_log_file)\n",
    "\n",
    "print(\"\\nAnalyzing S2 images...\")\n",
    "analyze_dataset(s2_image_dir, s2_log_file)\n",
    "\n",
    "print(\"Analysis completed. Check the log files for details.\")\n"
   ],
   "id": "dcc54e4736fa21e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN提取并拼接S1和S2数据特征",
   "id": "c8aa7760c9ae90ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将拼接特征保存到本地，避免重复计算和内存不足。",
   "id": "63df223bc20d926d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import rasterio\n",
    "from collections import defaultdict\n",
    "from torch.nn import Module, Conv2d, ReLU, Flatten, Linear\n",
    "\n",
    "\n",
    "# 定义一个简单的CNN模型\n",
    "class SimpleCNN(Module):\n",
    "    def __init__(self, in_channels, input_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 使用2x2卷积核\n",
    "        self.conv1 = Conv2d(in_channels, 64, kernel_size=2, stride=1, padding=0)\n",
    "        self.relu = ReLU()\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        # 计算卷积层输出的尺寸\n",
    "        conv_output_size = (input_size[0] - 1) * (input_size[1] - 1) * 64\n",
    "        self.fc1 = Linear(conv_output_size, 128)\n",
    "        self.fc2 = Linear(128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SampleSiteDataset:\n",
    "    def __init__(self, s1_image_dir, s2_image_dir, output_dir):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param s1_image_dir: S1影像根目录\n",
    "        :param s2_image_dir: S2影像根目录\n",
    "        :param output_dir: 特征保存目录\n",
    "        \"\"\"\n",
    "        self.s1_image_dir = s1_image_dir\n",
    "        self.s2_image_dir = s2_image_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 递归获取所有 .tif 文件\n",
    "        self.s1_images = self.get_all_tif_files(s1_image_dir)\n",
    "        self.s2_images = self.get_all_tif_files(s2_image_dir)\n",
    "\n",
    "        # 获取样地列表\n",
    "        s1_sample_sites = set(self.s1_images.keys())\n",
    "        s2_sample_sites = set(self.s2_images.keys())\n",
    "\n",
    "        # 取交集，确保只处理 S1 和 S2 都存在的样地\n",
    "        self.sample_sites = sorted(s1_sample_sites & s2_sample_sites)\n",
    "\n",
    "        # 打印样地列表\n",
    "        print(f\"S1 Sample Sites: {s1_sample_sites}\")\n",
    "        print(f\"S2 Sample Sites: {s2_sample_sites}\")\n",
    "        print(f\"Common Sample Sites: {self.sample_sites}\")\n",
    "\n",
    "        # 加载简单的CNN模型\n",
    "        with rasterio.open(next(iter(self.s1_images.values()))[0][1]) as src:\n",
    "            s1_input_size = (src.height, src.width)\n",
    "        with rasterio.open(next(iter(self.s2_images.values()))[0][1]) as src:\n",
    "            s2_input_size = (src.height, src.width)\n",
    "\n",
    "        self.cnn_s1 = SimpleCNN(in_channels=4, input_size=s1_input_size)  # 假设S1有4个通道\n",
    "        self.cnn_s2 = SimpleCNN(in_channels=19, input_size=s2_input_size)  # 假设S2有19个通道\n",
    "        self.cnn_s1.eval()\n",
    "        self.cnn_s2.eval()\n",
    "\n",
    "    def get_all_tif_files(self, root_dir):\n",
    "        tif_files = defaultdict(list)\n",
    "        for dirpath, _, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.lower().endswith('.tif'):\n",
    "                    sample_site = filename.split('_')[0]\n",
    "                    date = self.parse_date(filename)\n",
    "                    tif_files[sample_site].append((date, os.path.join(dirpath, filename)))\n",
    "        return tif_files\n",
    "\n",
    "    def parse_date(self, filename):\n",
    "        parts = filename.split('_')\n",
    "        date_str = parts[-1].split('.')[0]\n",
    "        return datetime.strptime(date_str, '%Y%m%d' if len(date_str) == 8 else '%Y-%m-%d')\n",
    "\n",
    "    def merge_monthly_images(self, image_paths):\n",
    "        with rasterio.open(image_paths[0][1]) as src:\n",
    "            profile = src.profile\n",
    "            data = np.zeros((src.count, src.height, src.width), dtype=np.float32)\n",
    "\n",
    "        for _, path in image_paths:\n",
    "            with rasterio.open(path) as src:\n",
    "                data += src.read()\n",
    "\n",
    "        data /= len(image_paths)\n",
    "        return data, profile\n",
    "\n",
    "    def extract_features(self, image_data):\n",
    "        # 检查图像数据的形状\n",
    "        print(f\"Image data shape: {image_data.shape}\")\n",
    "\n",
    "        # 确保图像数据的形状正确\n",
    "        if image_data.ndim != 3 or image_data.shape[0] not in [4, 19]:\n",
    "            raise ValueError(f\"Image data shape {image_data.shape} is not (C, H, W) with C in [4, 19].\")\n",
    "\n",
    "        # 直接将 image_data 转换为 torch.tensor，并添加一个批次维度\n",
    "        tensor = torch.tensor(image_data).unsqueeze(0)\n",
    "        print(f\"Tensor shape: {tensor.shape}\")\n",
    "\n",
    "        features = self.cnn_s1(tensor) if tensor.shape[1] == 4 else self.cnn_s2(tensor)\n",
    "        return features.squeeze(0).detach().numpy()\n",
    "\n",
    "    def save_features(self, sample_site, date, combined_features):\n",
    "        feature_file = os.path.join(self.output_dir, f\"{sample_site}_{date.strftime('%Y%m%d')}.npy\")\n",
    "        np.save(feature_file, combined_features)\n",
    "        print(f\"Saved features to {feature_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_sites)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_site = self.sample_sites[idx]\n",
    "\n",
    "        # 获取该样地的所有年份的S1和S2影像\n",
    "        s1_images = self.s1_images[sample_site]\n",
    "        s2_images = self.s2_images[sample_site]\n",
    "\n",
    "        # 按月度合并S1影像\n",
    "        s1_features = {}\n",
    "        for month, paths in self.group_by_month(s1_images).items():\n",
    "            merged_image, _ = self.merge_monthly_images(paths)\n",
    "            s1_features[month] = self.extract_features(merged_image)\n",
    "\n",
    "        # 提取S2影像的特征\n",
    "        s2_features = {}\n",
    "        for date, path in s2_images:\n",
    "            with rasterio.open(path) as src:\n",
    "                image_data = src.read()\n",
    "            s2_feature = self.extract_features(image_data)\n",
    "            s2_features[date] = (path, s2_feature)\n",
    "\n",
    "        # 拼接特征并保存\n",
    "        for month in s1_features:\n",
    "            for date, (path, s2_feature) in s2_features.items():\n",
    "                if date.strftime('%Y-%m') == month:\n",
    "                    combined_feature = np.concatenate((s1_features[month], s2_feature))\n",
    "                    self.save_features(sample_site, date, combined_feature)\n",
    "                    print(f\"Sample {idx}, Date {date}: S1 feature shape: {s1_features[month].shape}, S2 feature shape: {s2_feature.shape}, Combined feature shape: {combined_feature.shape}, S2 Image Path: {path}\")\n",
    "\n",
    "    def group_by_month(self, images):\n",
    "        grouped = defaultdict(list)\n",
    "        for date, path in images:\n",
    "            month = date.strftime('%Y-%m')\n",
    "            grouped[month].append((date, path))\n",
    "        return grouped\n",
    "\n",
    "# 使用示例\n",
    "s1_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S1_processed_cropped_index\"\n",
    "s2_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S2\\cropped\"\n",
    "output_dir = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "\n",
    "dataset = SampleSiteDataset(s1_image_dir, s2_image_dir, output_dir)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i]\n"
   ],
   "id": "3669ccd89db6a253"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 检查特征矩阵是否合理",
   "id": "ba0380407f782ba8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def check_npy_file(file_path):\n",
    "    try:\n",
    "        data = np.load(file_path)\n",
    "        if np.isnan(data).any() or np.isinf(data).any():\n",
    "            return True, file_path\n",
    "        else:\n",
    "            return False, file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, file_path\n",
    "\n",
    "\n",
    "def batch_check_npy_files(folder_path):\n",
    "    problematic_files = []\n",
    "\n",
    "    # 获取文件夹中所有.npy文件的路径\n",
    "    npy_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        has_issue, path = check_npy_file(file_path)\n",
    "        if has_issue is not None and has_issue:\n",
    "            problematic_files.append(path)\n",
    "\n",
    "    return problematic_files\n",
    "\n",
    "# 设置文件夹路径\n",
    "folder_path = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "\n",
    "# 检查并输出有问题的文件\n",
    "problematic_files = batch_check_npy_files(folder_path)\n",
    "\n",
    "if problematic_files:\n",
    "    print(\"The following files contain NaN or Inf values:\")\n",
    "    for file in problematic_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"All .npy files are clean.\")\n"
   ],
   "id": "c174f1d92bc985f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 提取日期信息到csv",
   "id": "85627d6b501980e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def extract_date_from_npy_filenames(feature_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    从 .npy 文件名中提取 sample_plot_id 和 date 信息，并保存到新的 CSV 文件中。\n",
    "\n",
    "    :param feature_dir: 包含 .npy 文件的目录路径\n",
    "    :param output_csv_path: 输出 CSV 文件的路径\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(feature_dir):\n",
    "        if filename.endswith('.npy'):\n",
    "            try:\n",
    "                # 解析文件名\n",
    "                sample_site, date_str = filename.split('_')\n",
    "                date_str = date_str.replace('.npy', '')\n",
    "                date = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "                # 添加到数据列表\n",
    "                data.append({\n",
    "                    'sample_plot_id': sample_site,\n",
    "                    'date': date.strftime('%Y-%m-%d')\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # 创建 DataFrame 并去重\n",
    "    df_dates = pd.DataFrame(data)\n",
    "    df_dates.drop_duplicates(inplace=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    df_dates.to_csv(output_csv_path, index=False, encoding='gbk')\n",
    "    print(f\"Date information saved to {output_csv_path}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    feature_dir = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "    output_csv_path = r\"E:\\postgraduate\\小论文\\小论文\\日期信息.csv\"\n",
    "    extract_date_from_npy_filenames(feature_dir, output_csv_path)\n"
   ],
   "id": "4ec7f9fda174b858"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 关联特征和树种标签",
   "id": "9e750c8aa02facda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将样地id、tip、date、影像特征矩阵文件位置关联起来，生成一个包含所有信息的表格。",
   "id": "ed987d9b414d78c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 文件路径\n",
    "csv_file_1 = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\npy日期信息.csv\"\n",
    "csv_file_2 = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\树种识别训练表格（保留关键特征列，手动聚类至20个）.csv\"\n",
    "output_csv = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features.csv\"\n",
    "npy_directory = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "\n",
    "# 读取第一个CSV文件中的sample_plot_id和date列\n",
    "df_date = pd.read_csv(csv_file_1, usecols=['sample_plot_id', 'date'], encoding=\"gbk\")\n",
    "\n",
    "# 将日期格式转换为统一格式 (YYYYMMDD)\n",
    "df_date['date'] = pd.to_datetime(df_date['date']).dt.strftime('%Y%m%d')\n",
    "\n",
    "# 读取第二个CSV文件中的sample_plot_id和tip列\n",
    "df_tip = pd.read_csv(csv_file_2, usecols=['sample_plot_id', 'tip'], encoding=\"gbk\")\n",
    "\n",
    "# 获取所有.npy文件的字典 {sample_plot_id: [npy文件列表]}\n",
    "npy_files_dict = {}\n",
    "for filename in os.listdir(npy_directory):\n",
    "    if filename.endswith('.npy'):\n",
    "        sample_plot_id, date_str = filename.split('_')\n",
    "        npy_files_dict.setdefault(sample_plot_id, []).append(os.path.join(npy_directory, filename))\n",
    "\n",
    "# 合并两个DataFrame，以sample_plot_id为键\n",
    "df_merged = df_date.merge(df_tip, on='sample_plot_id', how='inner')\n",
    "\n",
    "# 添加.npy文件位置列\n",
    "def get_npy_location(row):\n",
    "    sample_plot_id = row['sample_plot_id']\n",
    "    date_str = row['date']\n",
    "    if sample_plot_id in npy_files_dict:\n",
    "        for npy_file in npy_files_dict[sample_plot_id]:\n",
    "            if f\"_{date_str}.npy\" in npy_file:\n",
    "                return npy_file\n",
    "    return 'N/A'\n",
    "\n",
    "df_merged['npy_location'] = df_merged.apply(get_npy_location, axis=1)\n",
    "\n",
    "# 输出到新的CSV文件\n",
    "df_merged.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Merged data has been saved to {output_csv}\")\n"
   ],
   "id": "d3ecb38e87fe82cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "csv_path = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# 打印前20个 .npy 文件的特征维度\n",
    "for index, row in data.head(20).iterrows():\n",
    "    npy_location = row['npy_location']\n",
    "    features = np.load(npy_location)\n",
    "    print(f\"File: {npy_location}, Shape: {features.shape}\")\n"
   ],
   "id": "6abf2b3f4fb2bb13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e2d87211d2443a42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# 读取 .npy 文件\n",
    "file_path = r\"E:\\postgraduate\\小论文\\小论文\\features\\43-410141-N-00659_20181104.npy\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 打印数据\n",
    "print(data)\n"
   ],
   "id": "62f2adb1da5d2cbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 只使用默认参数进行分类训练",
   "id": "3876162a8c4d23ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class TipDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path, encoding='gbk')\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.data['tip'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_location = self.data.iloc[idx]['npy_location']\n",
    "        features = np.load(npy_location)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# 定义Transformer模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 512)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # 添加序列维度 (batch_size, seq_len, embed_dim)        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 超参数\n",
    "input_dim = 64  # 每个.npy文件的特征维度\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 64  # 设置batch_size\n",
    "\n",
    "# 数据加载\n",
    "csv_path = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features_2.csv\"\n",
    "dataset = TipDataset(csv_path)\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_dim, num_heads, num_layers, num_classes=len(dataset.label_encoder.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "def test(model, device, test_loader, criterion, label_encoder):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # 收集预测和真实标签\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
    "\n",
    "    # 打印每个类别的分类报告\n",
    "    class_names = label_encoder.classes_\n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names, zero_division=0)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # 打印混淆矩阵\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        accuracy = test(model, device, test_loader, criterion, dataset.label_encoder)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Best model saved with accuracy: {best_accuracy:.2f}%')"
   ],
   "id": "608322c99a81d3fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ac1da734128e014a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n"
   ],
   "id": "e06f133681521d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def check_invalid_values(image_path):\n",
    "    \"\"\"\n",
    "    检查单张影像是否存在无效值，并统计无效值比例。\n",
    "    :param image_path: 影像文件路径\n",
    "    :return: 无效值比例字典，键为波段索引，值为无效值比例\n",
    "    \"\"\"\n",
    "    invalid_ratios = {}\n",
    "    try:\n",
    "        with rasterio.open(image_path) as src:\n",
    "            data = src.read()  # 读取影像数据 (C, H, W)\n",
    "            num_bands, height, width = data.shape\n",
    "\n",
    "            for band_idx in range(num_bands):\n",
    "                band_data = data[band_idx]\n",
    "                total_pixels = height * width\n",
    "                invalid_pixels = np.isnan(band_data).sum() + np.isinf(band_data).sum()\n",
    "                invalid_ratio = invalid_pixels / total_pixels\n",
    "                invalid_ratios[band_idx + 1] = invalid_ratio  # 波段从1开始编号\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return invalid_ratios\n",
    "\n",
    "\n",
    "def analyze_dataset(root_dir, output_log):\n",
    "    \"\"\"\n",
    "    分析整个数据集，统计每张影像的无效值比例，并将结果写入日志文件。\n",
    "    :param root_dir: 数据集根目录\n",
    "    :param output_log: 输出日志文件路径\n",
    "    \"\"\"\n",
    "    invalid_images = []  # 存储包含无效值的影像路径\n",
    "    total_images = 0  # 总影像数\n",
    "\n",
    "    # 打开日志文件以写入模式\n",
    "    with open(output_log, \"w\") as log_file:\n",
    "        log_file.write(\"Invalid Value Analysis Report\\n\")\n",
    "        log_file.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        # 遍历目录中的所有 .tif 文件\n",
    "        for dirpath, _, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.lower().endswith('.tif'):\n",
    "                    total_images += 1\n",
    "                    image_path = os.path.join(dirpath, filename)\n",
    "                    invalid_ratios = check_invalid_values(image_path)\n",
    "\n",
    "                    if invalid_ratios is not None and any(ratio > 0 for ratio in invalid_ratios.values()):\n",
    "                        invalid_images.append((image_path, invalid_ratios))\n",
    "                        log_file.write(f\"Invalid values found in {image_path}:\\n\")\n",
    "                        for band, ratio in invalid_ratios.items():\n",
    "                            log_file.write(f\"  Band {band}: {ratio:.2%} invalid pixels\\n\")\n",
    "\n",
    "        # 写入总结信息\n",
    "        log_file.write(\"\\nAnalysis Summary:\\n\")\n",
    "        log_file.write(f\"Total images analyzed: {total_images}\\n\")\n",
    "        log_file.write(f\"Images with invalid values: {len(invalid_images)}\\n\")\n",
    "        if invalid_images:\n",
    "            log_file.write(\"List of images with invalid values:\\n\")\n",
    "            for path, _ in invalid_images:\n",
    "                log_file.write(f\"  {path}\\n\")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "# s1_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S1_processed_cropped_index\"\n",
    "s2_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S2\\ValidImages\"\n",
    "\n",
    "# 定义日志文件路径\n",
    "# s1_log_file = os.path.join(s1_image_dir, \"S1_invalid_values.txt\")\n",
    "s2_log_file = os.path.join(s2_image_dir, \"S2_invalid_values.txt\")\n",
    "\n",
    "# print(\"Analyzing S1 images...\")\n",
    "# analyze_dataset(s1_image_dir, s1_log_file)\n",
    "\n",
    "print(\"\\nAnalyzing S2 images...\")\n",
    "analyze_dataset(s2_image_dir, s2_log_file)\n",
    "\n",
    "print(\"Analysis completed. Check the log files for details.\")\n"
   ],
   "id": "bb17c1e820ee942c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN提取并拼接S1和S2数据特征",
   "id": "95edab80e285082c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将拼接特征保存到本地，避免重复计算和内存不足。",
   "id": "2cd001d571e0b450"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 提取3*3区域尺寸像素特征",
   "id": "eab545962566bf89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import rasterio\n",
    "from collections import defaultdict\n",
    "from torch.nn import Module, Conv2d, ReLU, Flatten, Linear\n",
    "\n",
    "\n",
    "# 定义一个简单的CNN模型\n",
    "class SimpleCNN(Module):\n",
    "    def __init__(self, in_channels, input_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 使用2x2卷积核\n",
    "        self.conv1 = Conv2d(in_channels, 64, kernel_size=2, stride=1, padding=0)\n",
    "        self.relu = ReLU()\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        # 计算卷积层输出的尺寸\n",
    "        conv_output_size = (input_size[0] - 1) * (input_size[1] - 1) * 64\n",
    "        self.fc1 = Linear(conv_output_size, 128)\n",
    "        self.fc2 = Linear(128, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SampleSiteDataset:\n",
    "    def __init__(self, s1_image_dir, s2_image_dir, output_dir):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param s1_image_dir: S1影像根目录\n",
    "        :param s2_image_dir: S2影像根目录\n",
    "        :param output_dir: 特征保存目录\n",
    "        \"\"\"\n",
    "        self.s1_image_dir = s1_image_dir\n",
    "        self.s2_image_dir = s2_image_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 递归获取所有 .tif 文件\n",
    "        self.s1_images = self.get_all_tif_files(s1_image_dir)\n",
    "        self.s2_images = self.get_all_tif_files(s2_image_dir)\n",
    "\n",
    "        # 获取样地列表\n",
    "        s1_sample_sites = set(self.s1_images.keys())\n",
    "        s2_sample_sites = set(self.s2_images.keys())\n",
    "\n",
    "        # 取交集，确保只处理 S1 和 S2 都存在的样地\n",
    "        self.sample_sites = sorted(s1_sample_sites & s2_sample_sites)\n",
    "\n",
    "        # 打印样地列表\n",
    "        print(f\"S1 Sample Sites: {s1_sample_sites}\")\n",
    "        print(f\"S2 Sample Sites: {s2_sample_sites}\")\n",
    "        print(f\"Common Sample Sites: {self.sample_sites}\")\n",
    "\n",
    "        # 加载简单的CNN模型\n",
    "        with rasterio.open(next(iter(self.s1_images.values()))[0][1]) as src:\n",
    "            s1_input_size = (src.height, src.width)\n",
    "        with rasterio.open(next(iter(self.s2_images.values()))[0][1]) as src:\n",
    "            s2_input_size = (src.height, src.width)\n",
    "\n",
    "        self.cnn_s1 = SimpleCNN(in_channels=4, input_size=s1_input_size)  # 假设S1有4个通道\n",
    "        self.cnn_s2 = SimpleCNN(in_channels=19, input_size=s2_input_size)  # 假设S2有19个通道\n",
    "        self.cnn_s1.eval()\n",
    "        self.cnn_s2.eval()\n",
    "\n",
    "    def get_all_tif_files(self, root_dir):\n",
    "        tif_files = defaultdict(list)\n",
    "        for dirpath, _, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.lower().endswith('.tif'):\n",
    "                    sample_site = filename.split('_')[0]\n",
    "                    date = self.parse_date(filename)\n",
    "                    tif_files[sample_site].append((date, os.path.join(dirpath, filename)))\n",
    "        return tif_files\n",
    "\n",
    "    def parse_date(self, filename):\n",
    "        parts = filename.split('_')\n",
    "        date_str = parts[-1].split('.')[0]\n",
    "        return datetime.strptime(date_str, '%Y%m%d' if len(date_str) == 8 else '%Y-%m-%d')\n",
    "\n",
    "    def merge_monthly_images(self, image_paths):\n",
    "        with rasterio.open(image_paths[0][1]) as src:\n",
    "            profile = src.profile\n",
    "            data = np.zeros((src.count, src.height, src.width), dtype=np.float32)\n",
    "\n",
    "        for _, path in image_paths:\n",
    "            with rasterio.open(path) as src:\n",
    "                data += src.read()\n",
    "\n",
    "        data /= len(image_paths)\n",
    "        return data, profile\n",
    "\n",
    "    def extract_features(self, image_data):\n",
    "        # 检查图像数据的形状\n",
    "        print(f\"Image data shape: {image_data.shape}\")\n",
    "\n",
    "        # 确保图像数据的形状正确\n",
    "        if image_data.ndim != 3 or image_data.shape[0] not in [4, 19]:\n",
    "            raise ValueError(f\"Image data shape {image_data.shape} is not (C, H, W) with C in [4, 19].\")\n",
    "\n",
    "        # 直接将 image_data 转换为 torch.tensor，并添加一个批次维度\n",
    "        tensor = torch.tensor(image_data).unsqueeze(0)\n",
    "        print(f\"Tensor shape: {tensor.shape}\")\n",
    "\n",
    "        features = self.cnn_s1(tensor) if tensor.shape[1] == 4 else self.cnn_s2(tensor)\n",
    "        return features.squeeze(0).detach().numpy()\n",
    "\n",
    "    def save_features(self, sample_site, date, combined_features):\n",
    "        feature_file = os.path.join(self.output_dir, f\"{sample_site}_{date.strftime('%Y%m%d')}.npy\")\n",
    "        np.save(feature_file, combined_features)\n",
    "        print(f\"Saved features to {feature_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_sites)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_site = self.sample_sites[idx]\n",
    "\n",
    "        # 获取该样地的所有年份的S1和S2影像\n",
    "        s1_images = self.s1_images[sample_site]\n",
    "        s2_images = self.s2_images[sample_site]\n",
    "\n",
    "        # 按月度合并S1影像\n",
    "        s1_features = {}\n",
    "        for month, paths in self.group_by_month(s1_images).items():\n",
    "            merged_image, _ = self.merge_monthly_images(paths)\n",
    "            s1_features[month] = self.extract_features(merged_image)\n",
    "\n",
    "        # 提取S2影像的特征\n",
    "        s2_features = {}\n",
    "        for date, path in s2_images:\n",
    "            with rasterio.open(path) as src:\n",
    "                image_data = src.read()\n",
    "            s2_feature = self.extract_features(image_data)\n",
    "            s2_features[date] = (path, s2_feature)\n",
    "\n",
    "        # 拼接特征并保存\n",
    "        for month in s1_features:\n",
    "            for date, (path, s2_feature) in s2_features.items():\n",
    "                if date.strftime('%Y-%m') == month:\n",
    "                    combined_feature = np.concatenate((s1_features[month], s2_feature))\n",
    "                    self.save_features(sample_site, date, combined_feature)\n",
    "                    print(f\"Sample {idx}, Date {date}: S1 feature shape: {s1_features[month].shape}, S2 feature shape: {s2_feature.shape}, Combined feature shape: {combined_feature.shape}, S2 Image Path: {path}\")\n",
    "\n",
    "    def group_by_month(self, images):\n",
    "        grouped = defaultdict(list)\n",
    "        for date, path in images:\n",
    "            month = date.strftime('%Y-%m')\n",
    "            grouped[month].append((date, path))\n",
    "        return grouped\n",
    "\n",
    "# 使用示例\n",
    "s1_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S1_processed_cropped_index\"\n",
    "s2_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S2\\cropped\"\n",
    "output_dir = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "\n",
    "dataset = SampleSiteDataset(s1_image_dir, s2_image_dir, output_dir)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i]\n"
   ],
   "id": "957913923d37b66d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 只简单提取提取中心像元特征并拼接S1\\S2数据",
   "id": "1104b266a9fc836d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import rasterio\n",
    "\n",
    "class SampleSiteDataset:\n",
    "    def __init__(self, s1_image_dir, s2_image_dir, output_dir):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param s1_image_dir: S1影像根目录\n",
    "        :param s2_image_dir: S2影像根目录\n",
    "        :param output_dir: 特征保存目录\n",
    "        \"\"\"\n",
    "        self.s1_image_dir = s1_image_dir\n",
    "        self.s2_image_dir = s2_image_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 递归获取所有 .tif 文件\n",
    "        self.s1_images = self.get_all_tif_files(s1_image_dir)\n",
    "        self.s2_images = self.get_all_tif_files(s2_image_dir)\n",
    "\n",
    "        # 获取样地列表\n",
    "        s1_sample_sites = set(self.s1_images.keys())\n",
    "        s2_sample_sites = set(self.s2_images.keys())\n",
    "\n",
    "        # 取交集，确保只处理 S1 和 S2 都存在的样地\n",
    "        self.sample_sites = sorted(s1_sample_sites & s2_sample_sites)\n",
    "\n",
    "        # 打印样地列表\n",
    "        print(f\"S1 Sample Sites: {s1_sample_sites}\")\n",
    "        print(f\"S2 Sample Sites: {s2_sample_sites}\")\n",
    "        print(f\"Common Sample Sites: {self.sample_sites}\")\n",
    "\n",
    "    def get_all_tif_files(self, root_dir):\n",
    "        tif_files = defaultdict(list)\n",
    "        for dirpath, _, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.lower().endswith('.tif'):\n",
    "                    sample_site = filename.split('_')[0]\n",
    "                    date = self.parse_date(filename)\n",
    "                    tif_files[sample_site].append((date, os.path.join(dirpath, filename)))\n",
    "        return tif_files\n",
    "\n",
    "    def parse_date(self, filename):\n",
    "        parts = filename.split('_')\n",
    "        date_str = parts[-1].split('.')[0]\n",
    "        return datetime.datetime.strptime(date_str, '%Y%m%d' if len(date_str) == 8 else '%Y-%m-%d')\n",
    "\n",
    "    def extract_center_pixel(self, image_data):\n",
    "        # 检查图像是否为3x3\n",
    "        if image_data.shape[1] != 3 or image_data.shape[2] != 3:\n",
    "            raise ValueError(\"Image size must be 3x3\")\n",
    "\n",
    "        # 提取中心像元的波段值\n",
    "        center_pixel_values = image_data[:, 1, 1]\n",
    "        return center_pixel_values\n",
    "\n",
    "    def save_features(self, sample_site, date, combined_features):\n",
    "        feature_file = os.path.join(self.output_dir, f\"{sample_site}_{date.strftime('%Y%m%d')}.npy\")\n",
    "        np.save(feature_file, combined_features)\n",
    "        print(f\"Saved features to {feature_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_sites)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_site = self.sample_sites[idx]\n",
    "\n",
    "        # 获取该样地的所有年份的S1和S2影像\n",
    "        s1_images = self.s1_images[sample_site]\n",
    "        s2_images = self.s2_images[sample_site]\n",
    "\n",
    "        # 按月度合并S1影像\n",
    "        s1_features = {}\n",
    "        for month, paths in self.group_by_month(s1_images).items():\n",
    "            merged_image, _ = self.merge_monthly_images(paths)\n",
    "            s1_features[month] = self.extract_center_pixel(merged_image)\n",
    "\n",
    "        # 提取S2影像的特征\n",
    "        s2_features = {}\n",
    "        for date, path in s2_images:\n",
    "            with rasterio.open(path) as src:\n",
    "                image_data = src.read()\n",
    "            s2_feature = self.extract_center_pixel(image_data)\n",
    "            s2_features[date] = (path, s2_feature)\n",
    "\n",
    "        # 拼接特征并保存\n",
    "        for month in s1_features:\n",
    "            for date, (path, s2_feature) in s2_features.items():\n",
    "                if date.strftime('%Y-%m') == month:\n",
    "                    combined_feature = np.concatenate((s1_features[month], s2_feature))\n",
    "                    self.save_features(sample_site, date, combined_feature)\n",
    "                    print(f\"Sample {idx}, Date {date}: S1 feature shape: {s1_features[month].shape}, S2 feature shape: {s2_feature.shape}, Combined feature shape: {combined_feature.shape}, S2 Image Path: {path}\")\n",
    "\n",
    "    def group_by_month(self, images):\n",
    "        grouped = defaultdict(list)\n",
    "        for date, path in images:\n",
    "            month = date.strftime('%Y-%m')\n",
    "            grouped[month].append((date, path))\n",
    "        return grouped\n",
    "\n",
    "    def merge_monthly_images(self, image_paths):\n",
    "        with rasterio.open(image_paths[0][1]) as src:\n",
    "            profile = src.profile\n",
    "            data = np.zeros((src.count, src.height, src.width), dtype=np.float32)\n",
    "\n",
    "        for _, path in image_paths:\n",
    "            with rasterio.open(path) as src:\n",
    "                data += src.read()\n",
    "\n",
    "        data /= len(image_paths)\n",
    "        return data, profile\n",
    "\n",
    "# 使用示例\n",
    "s1_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S1_processed_cropped_index\"\n",
    "s2_image_dir = r\"E:\\postgraduate\\小论文\\小论文\\S2\\ValidImages\"\n",
    "output_dir = r\"E:\\postgraduate\\小论文\\小论文\\features_(center)\"\n",
    "\n",
    "dataset = SampleSiteDataset(s1_image_dir, s2_image_dir, output_dir)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i]\n"
   ],
   "id": "61a700a67c7698b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 检查特征矩阵是否合理",
   "id": "b7fec770471d6bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def check_npy_file(file_path):\n",
    "    try:\n",
    "        data = np.load(file_path)\n",
    "        if np.isnan(data).any() or np.isinf(data).any():\n",
    "            return True, file_path\n",
    "        else:\n",
    "            return False, file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None, file_path\n",
    "\n",
    "\n",
    "def batch_check_npy_files(folder_path):\n",
    "    problematic_files = []\n",
    "\n",
    "    # 获取文件夹中所有.npy文件的路径\n",
    "    npy_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        has_issue, path = check_npy_file(file_path)\n",
    "        if has_issue is not None and has_issue:\n",
    "            problematic_files.append(path)\n",
    "\n",
    "    return problematic_files\n",
    "\n",
    "# 设置文件夹路径\n",
    "folder_path = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "\n",
    "# 检查并输出有问题的文件\n",
    "problematic_files = batch_check_npy_files(folder_path)\n",
    "\n",
    "if problematic_files:\n",
    "    print(\"The following files contain NaN or Inf values:\")\n",
    "    for file in problematic_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"All .npy files are clean.\")\n"
   ],
   "id": "d4560b7bf3ce7682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 提取日期信息到csv",
   "id": "582b2047225f54b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def extract_date_from_npy_filenames(feature_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    从 .npy 文件名中提取 sample_plot_id 和 date 信息，并保存到新的 CSV 文件中。\n",
    "\n",
    "    :param feature_dir: 包含 .npy 文件的目录路径\n",
    "    :param output_csv_path: 输出 CSV 文件的路径\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(feature_dir):\n",
    "        if filename.endswith('.npy'):\n",
    "            try:\n",
    "                # 解析文件名\n",
    "                sample_site, date_str = filename.split('_')\n",
    "                date_str = date_str.replace('.npy', '')\n",
    "                date = datetime.strptime(date_str, '%Y%m%d')\n",
    "\n",
    "                # 添加到数据列表\n",
    "                data.append({\n",
    "                    'sample_plot_id': sample_site,\n",
    "                    'date': date.strftime('%Y-%m-%d')\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # 创建 DataFrame 并去重\n",
    "    df_dates = pd.DataFrame(data)\n",
    "    df_dates.drop_duplicates(inplace=True)\n",
    "\n",
    "    # 保存到 CSV 文件\n",
    "    df_dates.to_csv(output_csv_path, index=False, encoding='gbk')\n",
    "    print(f\"Date information saved to {output_csv_path}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    feature_dir = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "    output_csv_path = r\"E:\\postgraduate\\小论文\\小论文\\日期信息.csv\"\n",
    "    extract_date_from_npy_filenames(feature_dir, output_csv_path)\n"
   ],
   "id": "612b38700dcf7be2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 关联特征和树种标签",
   "id": "dce13d4ad383695"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "将样地id、tip、date、影像特征矩阵文件位置关联起来，生成一个包含所有信息的表格。",
   "id": "a62243c585bdf232"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 文件路径\n",
    "csv_file_1 = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\npy日期信息.csv\"\n",
    "csv_file_2 = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\树种识别训练表格（保留关键特征列，手动聚类至20个）.csv\"\n",
    "output_csv = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features.csv\"\n",
    "npy_directory = r\"E:\\postgraduate\\小论文\\小论文\\features\"\n",
    "\n",
    "# 读取第一个CSV文件中的sample_plot_id和date列\n",
    "df_date = pd.read_csv(csv_file_1, usecols=['sample_plot_id', 'date'], encoding=\"gbk\")\n",
    "\n",
    "# 将日期格式转换为统一格式 (YYYYMMDD)\n",
    "df_date['date'] = pd.to_datetime(df_date['date']).dt.strftime('%Y%m%d')\n",
    "\n",
    "# 读取第二个CSV文件中的sample_plot_id和tip列\n",
    "df_tip = pd.read_csv(csv_file_2, usecols=['sample_plot_id', 'tip'], encoding=\"gbk\")\n",
    "\n",
    "# 获取所有.npy文件的字典 {sample_plot_id: [npy文件列表]}\n",
    "npy_files_dict = {}\n",
    "for filename in os.listdir(npy_directory):\n",
    "    if filename.endswith('.npy'):\n",
    "        sample_plot_id, date_str = filename.split('_')\n",
    "        npy_files_dict.setdefault(sample_plot_id, []).append(os.path.join(npy_directory, filename))\n",
    "\n",
    "# 合并两个DataFrame，以sample_plot_id为键\n",
    "df_merged = df_date.merge(df_tip, on='sample_plot_id', how='inner')\n",
    "\n",
    "# 添加.npy文件位置列\n",
    "def get_npy_location(row):\n",
    "    sample_plot_id = row['sample_plot_id']\n",
    "    date_str = row['date']\n",
    "    if sample_plot_id in npy_files_dict:\n",
    "        for npy_file in npy_files_dict[sample_plot_id]:\n",
    "            if f\"_{date_str}.npy\" in npy_file:\n",
    "                return npy_file\n",
    "    return 'N/A'\n",
    "\n",
    "df_merged['npy_location'] = df_merged.apply(get_npy_location, axis=1)\n",
    "\n",
    "# 输出到新的CSV文件\n",
    "df_merged.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Merged data has been saved to {output_csv}\")\n"
   ],
   "id": "d4ec73010d432f07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "csv_path = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# 打印前20个 .npy 文件的特征维度\n",
    "for index, row in data.head(20).iterrows():\n",
    "    npy_location = row['npy_location']\n",
    "    features = np.load(npy_location)\n",
    "    print(f\"File: {npy_location}, Shape: {features.shape}\")\n"
   ],
   "id": "b3e4272b74e7b5bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a1c9de1f6b077fb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# 读取 .npy 文件\n",
    "file_path = r\"E:\\postgraduate\\小论文\\小论文\\features\\43-410141-N-00659_20181104.npy\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "# 打印数据\n",
    "print(data)\n"
   ],
   "id": "a7673ed15e05f048"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 只使用默认参数进行分类训练",
   "id": "7b8241d46061e1ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练提取的3*3区域特征",
   "id": "330b67896acc8df5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class TipDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path, encoding='gbk')\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.data['tip'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_location = self.data.iloc[idx]['npy_location']\n",
    "        features = np.load(npy_location)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# 定义Transformer模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 512)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # 添加序列维度 (batch_size, seq_len, embed_dim)        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 超参数\n",
    "input_dim = 64  # 每个.npy文件的特征维度\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 64  # 设置batch_size\n",
    "\n",
    "# 数据加载\n",
    "csv_path = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features_2.csv\"\n",
    "dataset = TipDataset(csv_path)\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_dim, num_heads, num_layers, num_classes=len(dataset.label_encoder.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "def test(model, device, test_loader, criterion, label_encoder):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # 收集预测和真实标签\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
    "\n",
    "    # 打印每个类别的分类报告\n",
    "    class_names = label_encoder.classes_\n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names, zero_division=0)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # 打印混淆矩阵\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        accuracy = test(model, device, test_loader, criterion, dataset.label_encoder)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Best model saved with accuracy: {best_accuracy:.2f}%')"
   ],
   "id": "893fa6eb0ba5e1bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练提取的中心像元特征",
   "id": "174313159036650e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "10fbc15ffacddbbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class TipDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        # 自动检测文件编码\n",
    "        encoding = detect_encoding(csv_path)\n",
    "        self.data = pd.read_csv(csv_path, encoding=encoding)\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.data['tip'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_location = self.data.iloc[idx]['npy_location']\n",
    "        features = np.load(npy_location)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# 定义Transformer模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 512)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # 添加序列维度 (batch_size, seq_len, embed_dim)        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 超参数\n",
    "input_dim = 23  # 每个.npy文件的特征维度\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 64  # 设置batch_size\n",
    "\n",
    "# 数据加载\n",
    "csv_path = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features(center).csv\"\n",
    "dataset = TipDataset(csv_path)\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_dim, num_heads, num_layers, num_classes=len(dataset.label_encoder.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "def test(model, device, test_loader, criterion, label_encoder):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # 收集预测和真实标签\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
    "\n",
    "    # 打印每个类别的分类报告\n",
    "    class_names = label_encoder.classes_\n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names, zero_division=0)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # 打印混淆矩阵\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        accuracy = test(model, device, test_loader, criterion, dataset.label_encoder)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Best model saved with accuracy: {best_accuracy:.2f}%')"
   ],
   "id": "c7cb1a7709cf7da4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 分两个tip列训练",
   "id": "f1ae134c6f360f01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']\n",
    "\n",
    "# 定义数据集类\n",
    "class TipDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        # 自动检测文件编码\n",
    "        encoding = detect_encoding(csv_path)\n",
    "        self.data = pd.read_csv(csv_path, encoding=encoding)\n",
    "        self.transform = transform\n",
    "\n",
    "        # 对 tip1 和 tip2 分别进行编码\n",
    "        self.label_encoder_tip1 = LabelEncoder()\n",
    "        self.labels_tip1 = self.label_encoder_tip1.fit_transform(self.data['tip1'])\n",
    "\n",
    "        self.label_encoder_tip2 = LabelEncoder()\n",
    "        self.labels_tip2 = self.label_encoder_tip2.fit_transform(self.data['tip2'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        npy_location = self.data.iloc[idx]['npy_location']\n",
    "        features = np.load(npy_location)\n",
    "        label_tip1 = self.labels_tip1[idx]\n",
    "        label_tip2 = self.labels_tip2[idx]\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label_tip1, dtype=torch.long), torch.tensor(label_tip2, dtype=torch.long)\n",
    "\n",
    "# 定义Transformer模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, num_classes_tip1, num_classes_tip2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 512)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_tip1 = nn.Linear(512, num_classes_tip1)  # 输出 tip1 类别\n",
    "        self.fc_tip2 = nn.Linear(512, num_classes_tip2)  # 输出 tip2 类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # 添加序列维度 (batch_size, seq_len, embed_dim)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        out_tip1 = self.fc_tip1(x)\n",
    "        out_tip2 = self.fc_tip2(x)\n",
    "        return out_tip1, out_tip2\n",
    "\n",
    "# 超参数\n",
    "input_dim = 23  # 每个.npy文件的特征维度\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 64  # 设置batch_size\n",
    "\n",
    "# 数据加载\n",
    "csv_path = r\"E:\\postgraduate\\小论文\\小论文\\表格文件\\id_tip_date_features(center).csv\"\n",
    "dataset = TipDataset(csv_path)\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_dim, num_heads, num_layers,\n",
    "                         num_classes_tip1=len(dataset.label_encoder_tip1.classes_),\n",
    "                         num_classes_tip2=len(dataset.label_encoder_tip2.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target_tip1, target_tip2) in enumerate(train_loader):\n",
    "        data, target_tip1, target_tip2 = data.to(device), target_tip1.to(device), target_tip2.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_tip1, output_tip2 = model(data)\n",
    "        loss_tip1 = criterion(output_tip1, target_tip1)\n",
    "        loss_tip2 = criterion(output_tip2, target_tip2)\n",
    "        total_loss = loss_tip1 + loss_tip2  # 可以根据实际情况调整权重\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {total_loss.item():.6f}')\n",
    "\n",
    "# 测试模型\n",
    "def test(model, device, test_loader, criterion, label_encoder_tip1, label_encoder_tip2):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_tip1 = 0\n",
    "    correct_tip2 = 0\n",
    "    all_preds_tip1 = []\n",
    "    all_targets_tip1 = []\n",
    "    all_preds_tip2 = []\n",
    "    all_targets_tip2 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target_tip1, target_tip2 in test_loader:\n",
    "            data, target_tip1, target_tip2 = data.to(device), target_tip1.to(device), target_tip2.to(device)\n",
    "            output_tip1, output_tip2 = model(data)\n",
    "            loss_tip1 = criterion(output_tip1, target_tip1)\n",
    "            loss_tip2 = criterion(output_tip2, target_tip2)\n",
    "            test_loss += (loss_tip1 + loss_tip2).item()\n",
    "\n",
    "            pred_tip1 = output_tip1.argmax(dim=1, keepdim=True)\n",
    "            pred_tip2 = output_tip2.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            correct_tip1 += pred_tip1.eq(target_tip1.view_as(pred_tip1)).sum().item()\n",
    "            correct_tip2 += pred_tip2.eq(target_tip2.view_as(pred_tip2)).sum().item()\n",
    "\n",
    "            # 收集预测和真实标签\n",
    "            all_preds_tip1.extend(pred_tip1.cpu().numpy().flatten())\n",
    "            all_targets_tip1.extend(target_tip1.cpu().numpy())\n",
    "            all_preds_tip2.extend(pred_tip2.cpu().numpy().flatten())\n",
    "            all_targets_tip2.extend(target_tip2.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy_tip1 = 100. * correct_tip1 / len(test_loader.dataset)\n",
    "    accuracy_tip2 = 100. * correct_tip2 / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy tip1: {accuracy_tip1:.2f}%, Accuracy tip2: {accuracy_tip2:.2f}%\\n')\n",
    "\n",
    "    # 打印每个类别的分类报告\n",
    "    class_names_tip1 = label_encoder_tip1.classes_\n",
    "    report_tip1 = classification_report(all_targets_tip1, all_preds_tip1, target_names=class_names_tip1, zero_division=0)\n",
    "    print(\"Classification Report for tip1:\\n\", report_tip1)\n",
    "\n",
    "    class_names_tip2 = label_encoder_tip2.classes_\n",
    "    report_tip2 = classification_report(all_targets_tip2, all_preds_tip2, target_names=class_names_tip2, zero_division=0)\n",
    "    print(\"Classification Report for tip2:\\n\", report_tip2)\n",
    "\n",
    "    # 打印混淆矩阵\n",
    "    cm_tip1 = confusion_matrix(all_targets_tip1, all_preds_tip1)\n",
    "    print(\"Confusion Matrix for tip1:\\n\", cm_tip1)\n",
    "\n",
    "    cm_tip2 = confusion_matrix(all_targets_tip2, all_preds_tip2)\n",
    "    print(\"Confusion Matrix for tip2:\\n\", cm_tip2)\n",
    "\n",
    "    return accuracy_tip1, accuracy_tip2\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    best_accuracy_tip1 = 0.0\n",
    "    best_accuracy_tip2 = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        accuracy_tip1, accuracy_tip2 = test(model, device, test_loader, criterion, dataset.label_encoder_tip1, dataset.label_encoder_tip2)\n",
    "        if accuracy_tip1 > best_accuracy_tip1 or accuracy_tip2 > best_accuracy_tip2:\n",
    "            best_accuracy_tip1 = accuracy_tip1\n",
    "            best_accuracy_tip2 = accuracy_tip2\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Best model saved with accuracy tip1: {best_accuracy_tip1:.2f}%, accuracy tip2: {best_accuracy_tip2:.2f}%')\n"
   ],
   "id": "f0ac1de3667ce411"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
